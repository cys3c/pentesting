#!/usr/bin/env python3
# -*- coding: utf-8 -*-

__author__ = '0ren'

import click
import sys
import urllib.request
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
from urllib.parse import urljoin

__author__ = '0ren'

USER_AGENT = ''


def parse_robots(url, user_agent=USER_AGENT):
    headers = {'User-Agent': user_agent}
    request = urllib.request.Request(
        urljoin(url, 'robots.txt'), headers=headers)
    try:
        for line in urllib.request.urlopen(request):
            path = line.decode('utf8').strip()
            if path.startswith('Disallow'):
                yield path.split(':')[1].strip()
    except URLError as e:
        if hasattr(e, 'reason'):
            click.echo(
                'failed to reach the server. Why? {}'.format(
                    e.reason), file=sys.stderr)
        elif hasattr(e, 'code'):
            click.echo(
                'failed to fetch robots.txt ({})'.format(
                    e.code), file=sys.stderr)
        return False


@click.command(context_settings=dict(help_option_names=['-h', '--help']))
@click.option('-u', '--url', help='The target URL to tamper.', required=True)
@click.option('-v', '--verbose', is_flag=True,
              help='Enable verbose mode.', default=False)
def tamper(verbose, url):
    """Tamper HTTP methods from disallowed paths via robots.txt."""
    if len(sys.argv[1:]) == 0:
        raise click.UsageError(
            'empty args. Try \'{} --help\' for more info'.format(sys.argv[0]))
    for path in parse_robots(url):
        pass


if __name__ == '__main__':
    tamper()
